{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 207, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/genkit.ts"],"sourcesContent":["import {genkit} from 'genkit';\nimport {googleAI} from '@genkit-ai/googleai';\n\nexport const ai = genkit({\n  plugins: [googleAI()],\n  model: 'googleai/gemini-2.0-flash',\n});\n"],"names":[],"mappings":";;;AAAA;AAAA;AACA;AAAA;;;AAEO,MAAM,KAAK,CAAA,GAAA,uIAAA,CAAA,SAAM,AAAD,EAAE;IACvB,SAAS;QAAC,CAAA,GAAA,2KAAA,CAAA,WAAQ,AAAD;KAAI;IACrB,OAAO;AACT","debugId":null}},
    {"offset": {"line": 228, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/speech-to-text-flow.ts"],"sourcesContent":["\n'use server';\n/**\n * @fileOverview A speech-to-text transcription AI agent.\n *\n * - transcribeAudio - A function that handles audio transcription.\n * - TranscribeAudioInput - The input type for the transcribeAudio function.\n * - TranscribeAudioOutput - The return type for the transcribeAudio function.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\nimport {googleAI} from '@genkit-ai/googleai';\n\nconst TranscribeAudioInputSchema = z.object({\n  audioDataUri: z\n    .string()\n    .describe(\n      \"A recording of spoken audio, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'.\"\n    ),\n    model: z.string().optional().describe(\"The AI model to use for transcription.\"),\n});\nexport type TranscribeAudioInput = z.infer<typeof TranscribeAudioInputSchema>;\n\nconst TranscriptWordSchema = z.object({\n  text: z.string(),\n  start: z.number().describe('Start time of the word in seconds.'),\n  end: z.number().describe('End time of the word in seconds.'),\n});\n\nexport const TranscribeAudioOutputSchema = z.object({\n  transcript: z.array(TranscriptWordSchema).describe('The structured transcript with word timings.'),\n  text: z.string().describe('The full transcribed text as a single string.'),\n});\nexport type TranscribeAudioOutput = z.infer<typeof TranscribeAudioOutputSchema>;\n\n\nexport async function transcribeAudio(input: TranscribeAudioInput): Promise<TranscribeAudioOutput> {\n  return transcribeAudioFlow(input);\n}\n\nconst prompt = ai.definePrompt({\n  name: 'transcribeAudioPrompt',\n  input: {schema: TranscribeAudioInputSchema},\n  output: {schema: TranscribeAudioOutputSchema},\n  prompt: `You are a highly accurate audio transcription service that can understand both English and Khmer.\nTranscribe the following audio. The audio may contain both English and Khmer words. Transcribe the words in the language they are spoken.\nInclude punctuation and structure the output as a clean, readable text.\nProvide the full text and a structured transcript with precise word-level timestamps.\n\nAudio: {{media url=audioDataUri}}`,\n  model: 'googleai/gemini-1.5-flash-latest',\n});\n\nconst transcribeAudioFlow = ai.defineFlow(\n  {\n    name: 'transcribeAudioFlow',\n    inputSchema: TranscribeAudioInputSchema,\n    outputSchema: TranscribeAudioOutputSchema,\n  },\n  async (input) => {\n    const model = input.model ? googleAI.model(input.model) : undefined;\n    const {output} = await prompt(input, { model });\n    if (!output) {\n      throw new Error('Transcription failed: The model did not return any output.');\n    }\n    \n    // Post-process to ensure start and end times are numbers.\n    const processedTranscript = output.transcript.map(word => ({\n        ...word,\n        start: Number(word.start) || 0,\n        end: Number(word.end) || 0,\n    }));\n\n    return {\n        transcript: processedTranscript,\n        text: output.text,\n    };\n  }\n);\n"],"names":[],"mappings":";;;;;;AAEA;;;;;;CAMC,GAED;AACA;AAAA;AACA;AAAA;;;;;;;AAEA,MAAM,6BAA6B,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAC1C,cAAc,uIAAA,CAAA,IAAC,CACZ,MAAM,GACN,QAAQ,CACP;IAEF,OAAO,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,QAAQ,CAAC;AAC1C;AAGA,MAAM,uBAAuB,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IACpC,MAAM,uIAAA,CAAA,IAAC,CAAC,MAAM;IACd,OAAO,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;IAC3B,KAAK,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;AAC3B;AAEO,MAAM,8BAA8B,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAClD,YAAY,uIAAA,CAAA,IAAC,CAAC,KAAK,CAAC,sBAAsB,QAAQ,CAAC;IACnD,MAAM,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;AAC5B;AAIO,eAAe,gBAAgB,KAA2B;IAC/D,OAAO,oBAAoB;AAC7B;AAEA,MAAM,SAAS,mHAAA,CAAA,KAAE,CAAC,YAAY,CAAC;IAC7B,MAAM;IACN,OAAO;QAAC,QAAQ;IAA0B;IAC1C,QAAQ;QAAC,QAAQ;IAA2B;IAC5C,QAAQ,CAAC;;;;;iCAKsB,CAAC;IAChC,OAAO;AACT;AAEA,MAAM,sBAAsB,mHAAA,CAAA,KAAE,CAAC,UAAU,CACvC;IACE,MAAM;IACN,aAAa;IACb,cAAc;AAChB,GACA,OAAO;IACL,MAAM,QAAQ,MAAM,KAAK,GAAG,2KAAA,CAAA,WAAQ,CAAC,KAAK,CAAC,MAAM,KAAK,IAAI;IAC1D,MAAM,EAAC,MAAM,EAAC,GAAG,MAAM,OAAO,OAAO;QAAE;IAAM;IAC7C,IAAI,CAAC,QAAQ;QACX,MAAM,IAAI,MAAM;IAClB;IAEA,0DAA0D;IAC1D,MAAM,sBAAsB,OAAO,UAAU,CAAC,GAAG,CAAC,CAAA,OAAQ,CAAC;YACvD,GAAG,IAAI;YACP,OAAO,OAAO,KAAK,KAAK,KAAK;YAC7B,KAAK,OAAO,KAAK,GAAG,KAAK;QAC7B,CAAC;IAED,OAAO;QACH,YAAY;QACZ,MAAM,OAAO,IAAI;IACrB;AACF;;;IAhDW;IAOS;;AAPT,+OAAA;AAOS,+OAAA","debugId":null}},
    {"offset": {"line": 319, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/improve-transcription-accuracy-flow.ts"],"sourcesContent":["'use server';\n/**\n * @fileOverview An AI agent for improving audio transcription accuracy using custom vocabulary.\n *\n * - improveTranscriptionAccuracy - A function that handles the transcription improvement process.\n * - ImproveTranscriptionAccuracyInput - The input type for the improveTranscriptionAccuracy function.\n * - TranscribeAudioOutput - The return type, shared with the standard speech-to-text flow.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\nimport {googleAI} from '@genkit-ai/googleai';\nimport type {TranscribeAudioOutput} from './speech-to-text-flow';\nimport {TranscribeAudioOutputSchema} from './speech-to-text-flow';\n\nconst ImproveTranscriptionAccuracyInputSchema = z.object({\n  audioDataUri: z\n    .string()\n    .describe(\n      \"A recording of spoken audio, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'.\"\n    ),\n  model: z.string().optional().describe(\"The AI model to use for transcription.\"),\n  customVocabulary: z.array(z.string()).describe(\"A list of custom words or phrases to improve recognition accuracy.\"),\n});\nexport type ImproveTranscriptionAccuracyInput = z.infer<typeof ImproveTranscriptionAccuracyInputSchema>;\n\nexport async function improveTranscriptionAccuracy(input: ImproveTranscriptionAccuracyInput): Promise<TranscribeAudioOutput> {\n  return improveTranscriptionAccuracyFlow(input);\n}\n\nconst prompt = ai.definePrompt({\n  name: 'improveTranscriptionAccuracyPrompt',\n  input: {schema: ImproveTranscriptionAccuracyInputSchema},\n  output: {schema: TranscribeAudioOutputSchema},\n  prompt: `You are a highly accurate audio transcription service that can understand both English and Khmer.\nTranscribe the following audio. The audio may contain both English and Khmer words. Transcribe the words in the language they are spoken.\nInclude punctuation and structure the output as a clean, readable text.\nProvide the full text and a structured transcript with precise word-level timestamps.\n\nPay special attention to the following custom vocabulary. Ensure these words are transcribed correctly if they appear in the audio:\n{{#each customVocabulary}}\n- {{{this}}}\n{{/each}}\n\nAudio: {{media url=audioDataUri}}`,\n  model: 'googleai/gemini-1.5-flash-latest',\n});\n\nconst improveTranscriptionAccuracyFlow = ai.defineFlow(\n  {\n    name: 'improveTranscriptionAccuracyFlow',\n    inputSchema: ImproveTranscriptionAccuracyInputSchema,\n    outputSchema: TranscribeAudioOutputSchema,\n  },\n  async (input) => {\n    const model = input.model ? googleAI.model(input.model) : undefined;\n    const {output} = await prompt(input, { model });\n    if (!output) {\n      throw new Error('Transcription failed: The model did not return any output.');\n    }\n    \n    // Post-process to ensure start and end times are numbers.\n    const processedTranscript = output.transcript.map(word => ({\n        ...word,\n        start: Number(word.start) || 0,\n        end: Number(word.end) || 0,\n    }));\n\n    return {\n        transcript: processedTranscript,\n        text: output.text,\n    };\n  }\n);\n"],"names":[],"mappings":";;;;;AACA;;;;;;CAMC,GAED;AACA;AAAA;AACA;AAAA;AAEA;;;;;;;;AAEA,MAAM,0CAA0C,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IACvD,cAAc,uIAAA,CAAA,IAAC,CACZ,MAAM,GACN,QAAQ,CACP;IAEJ,OAAO,uIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,QAAQ,CAAC;IACtC,kBAAkB,uIAAA,CAAA,IAAC,CAAC,KAAK,CAAC,uIAAA,CAAA,IAAC,CAAC,MAAM,IAAI,QAAQ,CAAC;AACjD;AAGO,eAAe,6BAA6B,KAAwC;IACzF,OAAO,iCAAiC;AAC1C;AAEA,MAAM,SAAS,mHAAA,CAAA,KAAE,CAAC,YAAY,CAAC;IAC7B,MAAM;IACN,OAAO;QAAC,QAAQ;IAAuC;IACvD,QAAQ;QAAC,QAAQ,kJAAA,CAAA,8BAA2B;IAAA;IAC5C,QAAQ,CAAC;;;;;;;;;;iCAUsB,CAAC;IAChC,OAAO;AACT;AAEA,MAAM,mCAAmC,mHAAA,CAAA,KAAE,CAAC,UAAU,CACpD;IACE,MAAM;IACN,aAAa;IACb,cAAc,kJAAA,CAAA,8BAA2B;AAC3C,GACA,OAAO;IACL,MAAM,QAAQ,MAAM,KAAK,GAAG,2KAAA,CAAA,WAAQ,CAAC,KAAK,CAAC,MAAM,KAAK,IAAI;IAC1D,MAAM,EAAC,MAAM,EAAC,GAAG,MAAM,OAAO,OAAO;QAAE;IAAM;IAC7C,IAAI,CAAC,QAAQ;QACX,MAAM,IAAI,MAAM;IAClB;IAEA,0DAA0D;IAC1D,MAAM,sBAAsB,OAAO,UAAU,CAAC,GAAG,CAAC,CAAA,OAAQ,CAAC;YACvD,GAAG,IAAI;YACP,OAAO,OAAO,KAAK,KAAK,KAAK;YAC7B,KAAK,OAAO,KAAK,GAAG,KAAK;QAC7B,CAAC;IAED,OAAO;QACH,YAAY;QACZ,MAAM,OAAO,IAAI;IACrB;AACF;;;IA9CoB;;AAAA,+OAAA","debugId":null}},
    {"offset": {"line": 406, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/.next-internal/server/app/page/actions.js%20%28server%20actions%20loader%29"],"sourcesContent":["export {transcribeAudio as '4021eec7f36ae423a90a123d847a05761fe9cb9e78'} from 'ACTIONS_MODULE0'\nexport {improveTranscriptionAccuracy as '40ae144d7927c19ed0628dfaa18854e94d6edf9419'} from 'ACTIONS_MODULE1'\n"],"names":[],"mappings":";AAAA;AACA","debugId":null}},
    {"offset": {"line": 464, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/components/voice-scribe-page.tsx/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport const VoiceScribePage = registerClientReference(\n    function() { throw new Error(\"Attempted to call VoiceScribePage() from the server but VoiceScribePage is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/components/voice-scribe-page.tsx <module evaluation>\",\n    \"VoiceScribePage\",\n);\n"],"names":[],"mappings":";;;AAAA;;AACO,MAAM,kBAAkB,CAAA,GAAA,qPAAA,CAAA,0BAAuB,AAAD,EACjD;IAAa,MAAM,IAAI,MAAM;AAA8O,GAC3Q,sEACA","debugId":null}},
    {"offset": {"line": 478, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/components/voice-scribe-page.tsx/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport const VoiceScribePage = registerClientReference(\n    function() { throw new Error(\"Attempted to call VoiceScribePage() from the server but VoiceScribePage is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/components/voice-scribe-page.tsx\",\n    \"VoiceScribePage\",\n);\n"],"names":[],"mappings":";;;AAAA;;AACO,MAAM,kBAAkB,CAAA,GAAA,qPAAA,CAAA,0BAAuB,AAAD,EACjD;IAAa,MAAM,IAAI,MAAM;AAA8O,GAC3Q,kDACA","debugId":null}},
    {"offset": {"line": 492, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 502, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/app/page.tsx"],"sourcesContent":["import { VoiceScribePage } from '@/components/voice-scribe-page';\n\nexport default function Home() {\n  return <VoiceScribePage />;\n}\n"],"names":[],"mappings":";;;;AAAA;;;AAEe,SAAS;IACtB,qBAAO,8OAAC,6IAAA,CAAA,kBAAe;;;;;AACzB","debugId":null}}]
}