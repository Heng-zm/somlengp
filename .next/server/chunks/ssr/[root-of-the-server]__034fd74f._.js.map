{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 207, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/genkit.ts"],"sourcesContent":["import {genkit} from 'genkit';\nimport {googleAI} from '@genkit-ai/googleai';\n\nexport const ai = genkit({\n  plugins: [\n    googleAI({\n      model: 'gemini-1.5-flash',\n    }),\n  ],\n});\n"],"names":[],"mappings":";;;AAAA;AAAA;AACA;AAAA;;;AAEO,MAAM,KAAK,CAAA,GAAA,uIAAA,CAAA,SAAM,AAAD,EAAE;IACvB,SAAS;QACP,CAAA,GAAA,2KAAA,CAAA,WAAQ,AAAD,EAAE;YACP,OAAO;QACT;KACD;AACH","debugId":null}},
    {"offset": {"line": 229, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/lib/types.ts"],"sourcesContent":["import { z } from 'zod';\n\nexport type TranscriptWord = {\n  text: string;\n  start: number;\n  end: number;\n};\n\nconst TranscriptWordSchema = z.object({\n  text: z.string(),\n  start: z.number().describe('Start time of the word in seconds.'),\n  end: z.number().describe('End time of the word in seconds.'),\n});\n\nexport const TranscribeAudioOutputSchema = z.object({\n  transcript: z\n    .array(TranscriptWordSchema)\n    .describe('The structured transcript with word timings.'),\n  text: z.string().describe('The full transcribed text as a single string.'),\n});\nexport type TranscribeAudioOutput = z.infer<\n  typeof TranscribeAudioOutputSchema\n>;\n"],"names":[],"mappings":";;;AAAA;;AAQA,MAAM,uBAAuB,oIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IACpC,MAAM,oIAAA,CAAA,IAAC,CAAC,MAAM;IACd,OAAO,oIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;IAC3B,KAAK,oIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;AAC3B;AAEO,MAAM,8BAA8B,oIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAClD,YAAY,oIAAA,CAAA,IAAC,CACV,KAAK,CAAC,sBACN,QAAQ,CAAC;IACZ,MAAM,oIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;AAC5B","debugId":null}},
    {"offset": {"line": 249, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/config.ts"],"sourcesContent":["// src/config.ts\n\n/**\n * The maximum file size in megabytes (MB) allowed for uploads.\n * This limit is enforced on the client-side before uploading\n * and on the server-side upon receiving the file.\n * NOTE: Vercel Hobby plan has a 4.5MB body size limit for Serverless Functions.\n * Base64 encoding adds ~37% overhead. To stay safely under 4.5MB,\n * we set the raw file limit to 3MB (3MB * 1.37 â‰ˆ 4.11MB).\n * The user has requested to increase this to 8MB. This may cause issues on\n * hosting platforms with smaller limits.\n */\nexport const MAX_FILE_SIZE_MB = 8;\n\n/**\n * The maximum file size in bytes.\n * Derived from MAX_FILE_SIZE_MB.\n */\nexport const MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024;\n\n/**\n * The maximum size for a base64-encoded string, accounting for the ~37% overhead.\n * This is used for server-side validation in Genkit flows.\n */\nexport const MAX_BASE64_SIZE_BYTES = MAX_FILE_SIZE_BYTES * 1.37;\n"],"names":[],"mappings":"AAAA,gBAAgB;AAEhB;;;;;;;;;CASC;;;;;AACM,MAAM,mBAAmB;AAMzB,MAAM,sBAAsB,mBAAmB,OAAO;AAMtD,MAAM,wBAAwB,sBAAsB","debugId":null}},
    {"offset": {"line": 273, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/speech-to-text-flow.ts"],"sourcesContent":["\n'use server';\n/**\n * @fileOverview A speech-to-text transcription AI agent.\n *\n * - transcribeAudio - A function that handles audio transcription.\n * - TranscribeAudioInput - The input type for the transcribeAudio function.\n * - TranscribeAudioOutput - The return type for the transcribeAudio function.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {googleAI} from '@genkit-ai/googleai';\nimport {z} from 'genkit';\nimport { TranscribeAudioOutput, TranscribeAudioOutputSchema } from '@/lib/types';\nimport { MAX_BASE64_SIZE_BYTES } from '@/config';\n\nconst TranscribeAudioInputSchema = z.object({\n  audioDataUri: z\n    .string()\n    .describe(\n      \"A recording of spoken audio, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'.\"\n    ),\n});\nexport type TranscribeAudioInput = z.infer<typeof TranscribeAudioInputSchema>;\n\n\nexport async function transcribeAudio(input: TranscribeAudioInput): Promise<TranscribeAudioOutput> {\n  return transcribeAudioFlow(input);\n}\n\nconst prompt = ai.definePrompt({\n  name: 'transcribeAudioPrompt',\n  model: googleAI.model('gemini-1.5-flash'),\n  input: {schema: TranscribeAudioInputSchema},\n  output: {schema: TranscribeAudioOutputSchema},\n  prompt: `You are a highly accurate audio transcription service that can understand both English and Khmer.\nTranscribe the following audio. The audio may contain both English and Khmer words. Transcribe the words in the language they are spoken.\nInclude punctuation and structure the output as a clean, readable text.\nProvide the full text and a structured transcript with precise word-level timestamps.\n\nAudio: {{media url=audioDataUri}}`,\n});\n\nconst transcribeAudioFlow = ai.defineFlow(\n  {\n    name: 'transcribeAudioFlow',\n    inputSchema: TranscribeAudioInputSchema,\n    outputSchema: TranscribeAudioOutputSchema,\n  },\n  async (input) => {\n    if (input.audioDataUri.length > MAX_BASE64_SIZE_BYTES) {\n        throw new Error('413: Payload Too Large. Audio file size exceeds the server limit.');\n    }\n    const {output} = await prompt(input);\n    if (!output) {\n      throw new Error('Transcription failed: The model did not return any output.');\n    }\n    \n    // Post-process to ensure start and end times are numbers.\n    const processedTranscript = output.transcript.map(word => ({\n        ...word,\n        start: Number(word.start) || 0,\n        end: Number(word.end) || 0,\n    }));\n\n    return {\n        transcript: processedTranscript,\n        text: output.text,\n    };\n  }\n);\n"],"names":[],"mappings":";;;;;AAEA;;;;;;CAMC,GAED;AACA;AAAA;AACA;AAAA;AACA;AACA;;;;;;;;;AAEA,MAAM,6BAA6B,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAC1C,cAAc,uIAAA,CAAA,IAAC,CACZ,MAAM,GACN,QAAQ,CACP;AAEN;AAIO,eAAe,gBAAgB,KAA2B;IAC/D,OAAO,oBAAoB;AAC7B;AAEA,MAAM,SAAS,mHAAA,CAAA,KAAE,CAAC,YAAY,CAAC;IAC7B,MAAM;IACN,OAAO,2KAAA,CAAA,WAAQ,CAAC,KAAK,CAAC;IACtB,OAAO;QAAC,QAAQ;IAA0B;IAC1C,QAAQ;QAAC,QAAQ,mHAAA,CAAA,8BAA2B;IAAA;IAC5C,QAAQ,CAAC;;;;;iCAKsB,CAAC;AAClC;AAEA,MAAM,sBAAsB,mHAAA,CAAA,KAAE,CAAC,UAAU,CACvC;IACE,MAAM;IACN,aAAa;IACb,cAAc,mHAAA,CAAA,8BAA2B;AAC3C,GACA,OAAO;IACL,IAAI,MAAM,YAAY,CAAC,MAAM,GAAG,6GAAA,CAAA,wBAAqB,EAAE;QACnD,MAAM,IAAI,MAAM;IACpB;IACA,MAAM,EAAC,MAAM,EAAC,GAAG,MAAM,OAAO;IAC9B,IAAI,CAAC,QAAQ;QACX,MAAM,IAAI,MAAM;IAClB;IAEA,0DAA0D;IAC1D,MAAM,sBAAsB,OAAO,UAAU,CAAC,GAAG,CAAC,CAAA,OAAQ,CAAC;YACvD,GAAG,IAAI;YACP,OAAO,OAAO,KAAK,KAAK,KAAK;YAC7B,KAAK,OAAO,KAAK,GAAG,KAAK;QAC7B,CAAC;IAED,OAAO;QACH,YAAY;QACZ,MAAM,OAAO,IAAI;IACrB;AACF;;;IA3CoB;;AAAA,+OAAA","debugId":null}},
    {"offset": {"line": 355, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/ai/flows/improve-transcription-accuracy-flow.ts"],"sourcesContent":["\n'use server';\n/**\n * @fileOverview An AI agent for improving audio transcription accuracy using custom vocabulary.\n *\n * - improveTranscriptionAccuracy - A function that handles the transcription improvement process.\n * - ImproveTranscriptionAccuracyInput - The input type for the improveTranscriptionAccuracy function.\n * - TranscribeAudioOutput - The return type, shared with the standard speech-to-text flow.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {googleAI} from '@genkit-ai/googleai';\nimport {z} from 'genkit';\nimport type {TranscribeAudioOutput} from '@/lib/types';\nimport {TranscribeAudioOutputSchema} from '@/lib/types';\nimport { MAX_BASE64_SIZE_BYTES } from '@/config';\n\nconst ImproveTranscriptionAccuracyInputSchema = z.object({\n  audioDataUri: z\n    .string()\n    .describe(\n      \"A recording of spoken audio, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'.\"\n    ),\n  customVocabulary: z.array(z.string()).describe(\"A list of custom words or phrases to improve recognition accuracy.\"),\n});\nexport type ImproveTranscriptionAccuracyInput = z.infer<typeof ImproveTranscriptionAccuracyInputSchema>;\n\nexport async function improveTranscriptionAccuracy(input: ImproveTranscriptionAccuracyInput): Promise<TranscribeAudioOutput> {\n  return improveTranscriptionAccuracyFlow(input);\n}\n\nconst prompt = ai.definePrompt({\n  name: 'improveTranscriptionAccuracyPrompt',\n  model: googleAI.model('gemini-1.5-flash'),\n  input: {schema: ImproveTranscriptionAccuracyInputSchema},\n  output: {schema: TranscribeAudioOutputSchema},\n  prompt: `You are a highly accurate audio transcription service that can understand both English and Khmer.\nTranscribe the following audio. The audio may contain both English and Khmer words. Transcribe the words in the language they are spoken.\nInclude punctuation and structure the output as a clean, readable text.\nProvide the full text and a structured transcript with precise word-level timestamps.\n\nPay special attention to the following custom vocabulary. Ensure these words are transcribed correctly if they appear in the audio:\n{{#each customVocabulary}}\n- {{{this}}}\n{{/each}}\n\nAudio: {{media url=audioDataUri}}`,\n});\n\nconst improveTranscriptionAccuracyFlow = ai.defineFlow(\n  {\n    name: 'improveTranscriptionAccuracyFlow',\n    inputSchema: ImproveTranscriptionAccuracyInputSchema,\n    outputSchema: TranscribeAudioOutputSchema,\n  },\n  async (input) => {\n    if (input.audioDataUri.length > MAX_BASE64_SIZE_BYTES) {\n        throw new Error('413: Payload Too Large. Audio file size exceeds the server limit.');\n    }\n    const {output} = await prompt(input);\n    if (!output) {\n      throw new Error('Transcription failed: The model did not return any output.');\n    }\n    \n    // Post-process to ensure start and end times are numbers.\n    const processedTranscript = output.transcript.map(word => ({\n        ...word,\n        start: Number(word.start) || 0,\n        end: Number(word.end) || 0,\n    }));\n\n    return {\n        transcript: processedTranscript,\n        text: output.text,\n    };\n  }\n);\n"],"names":[],"mappings":";;;;;AAEA;;;;;;CAMC,GAED;AACA;AAAA;AACA;AAAA;AAEA;AACA;;;;;;;;;AAEA,MAAM,0CAA0C,uIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IACvD,cAAc,uIAAA,CAAA,IAAC,CACZ,MAAM,GACN,QAAQ,CACP;IAEJ,kBAAkB,uIAAA,CAAA,IAAC,CAAC,KAAK,CAAC,uIAAA,CAAA,IAAC,CAAC,MAAM,IAAI,QAAQ,CAAC;AACjD;AAGO,eAAe,6BAA6B,KAAwC;IACzF,OAAO,iCAAiC;AAC1C;AAEA,MAAM,SAAS,mHAAA,CAAA,KAAE,CAAC,YAAY,CAAC;IAC7B,MAAM;IACN,OAAO,2KAAA,CAAA,WAAQ,CAAC,KAAK,CAAC;IACtB,OAAO;QAAC,QAAQ;IAAuC;IACvD,QAAQ;QAAC,QAAQ,mHAAA,CAAA,8BAA2B;IAAA;IAC5C,QAAQ,CAAC;;;;;;;;;;iCAUsB,CAAC;AAClC;AAEA,MAAM,mCAAmC,mHAAA,CAAA,KAAE,CAAC,UAAU,CACpD;IACE,MAAM;IACN,aAAa;IACb,cAAc,mHAAA,CAAA,8BAA2B;AAC3C,GACA,OAAO;IACL,IAAI,MAAM,YAAY,CAAC,MAAM,GAAG,6GAAA,CAAA,wBAAqB,EAAE;QACnD,MAAM,IAAI,MAAM;IACpB;IACA,MAAM,EAAC,MAAM,EAAC,GAAG,MAAM,OAAO;IAC9B,IAAI,CAAC,QAAQ;QACX,MAAM,IAAI,MAAM;IAClB;IAEA,0DAA0D;IAC1D,MAAM,sBAAsB,OAAO,UAAU,CAAC,GAAG,CAAC,CAAA,OAAQ,CAAC;YACvD,GAAG,IAAI;YACP,OAAO,OAAO,KAAK,KAAK,KAAK;YAC7B,KAAK,OAAO,KAAK,GAAG,KAAK;QAC7B,CAAC;IAED,OAAO;QACH,YAAY;QACZ,MAAM,OAAO,IAAI;IACrB;AACF;;;IAhDoB;;AAAA,+OAAA","debugId":null}},
    {"offset": {"line": 443, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/.next-internal/server/app/voice-transcript/page/actions.js%20%28server%20actions%20loader%29"],"sourcesContent":["export {transcribeAudio as '4021eec7f36ae423a90a123d847a05761fe9cb9e78'} from 'ACTIONS_MODULE0'\nexport {improveTranscriptionAccuracy as '40ae144d7927c19ed0628dfaa18854e94d6edf9419'} from 'ACTIONS_MODULE1'\n"],"names":[],"mappings":";AAAA;AACA","debugId":null}},
    {"offset": {"line": 501, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/app/voice-transcript/page.tsx/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport default registerClientReference(\n    function() { throw new Error(\"Attempted to call the default export of [project]/src/app/voice-transcript/page.tsx <module evaluation> from the server, but it's on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/app/voice-transcript/page.tsx <module evaluation>\",\n    \"default\",\n);\n"],"names":[],"mappings":";;;AAAA;;uCACe,CAAA,GAAA,qPAAA,CAAA,0BAAuB,AAAD,EACjC;IAAa,MAAM,IAAI,MAAM;AAAqS,GAClU,mEACA","debugId":null}},
    {"offset": {"line": 515, "column": 0}, "map": {"version":3,"sources":["file:///home/user/studio/src/app/voice-transcript/page.tsx/proxy.mjs"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport default registerClientReference(\n    function() { throw new Error(\"Attempted to call the default export of [project]/src/app/voice-transcript/page.tsx from the server, but it's on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/src/app/voice-transcript/page.tsx\",\n    \"default\",\n);\n"],"names":[],"mappings":";;;AAAA;;uCACe,CAAA,GAAA,qPAAA,CAAA,0BAAuB,AAAD,EACjC;IAAa,MAAM,IAAI,MAAM;AAAiR,GAC9S,+CACA","debugId":null}},
    {"offset": {"line": 529, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}}]
}